{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\netscratch\\dmittal\\continual_learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "# current_directory = \n",
    "\n",
    "# Replace forward slashes with backslashes\n",
    "converted_directory = os.getcwd().replace(\"/\", \"\\\\\")\n",
    "\n",
    "# Print the result\n",
    "print(converted_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : realworld\n",
      "Number of Classes: 8\n",
      "All Classes:  \t[0 1 2 3 4 5 6 7]\n",
      "\n",
      "Inital Step Classes: [0 1 2 3 4 5]\n",
      "Incremental Step New Classes: [6 7]\n",
      "Subjects:  [np.str_('imu\\\\1'), np.str_('imu\\\\10'), np.str_('imu\\\\11'), np.str_('imu\\\\12'), np.str_('imu\\\\13'), np.str_('imu\\\\14'), np.str_('imu\\\\15'), np.str_('imu\\\\2'), np.str_('imu\\\\3'), np.str_('imu\\\\4'), np.str_('imu\\\\5'), np.str_('imu\\\\6'), np.str_('imu\\\\7'), np.str_('imu\\\\8'), np.str_('imu\\\\9')]\n",
      "-------------------------\n",
      "\n",
      "Fold 0, Train Subjects: ['imu\\\\10', 'imu\\\\11', 'imu\\\\12', 'imu\\\\13', 'imu\\\\14', 'imu\\\\15', 'imu\\\\2', 'imu\\\\3', 'imu\\\\5', 'imu\\\\7', 'imu\\\\8', 'imu\\\\9'], Test Subjects: ['imu\\\\1', 'imu\\\\4', 'imu\\\\6']\n",
      "Total Data: Seen: 74555, Unseen: 30657\n",
      "Inital\n",
      "Train: Len: 53679 , Unique: 6 People: ['imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15' 'imu\\\\2'\n",
      " 'imu\\\\3' 'imu\\\\5' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9']\n",
      "Val: Len: 5965, Unique: 6 People: ['imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15' 'imu\\\\2'\n",
      " 'imu\\\\3' 'imu\\\\5' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9']\n",
      "Test: Len: 19854, Unique: 6 People: ['imu\\\\1' 'imu\\\\4' 'imu\\\\6']\n",
      "Incremantal\n",
      "Train: Len: 41010 , Unique: 8, People: ['imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15' 'imu\\\\2'\n",
      " 'imu\\\\3' 'imu\\\\5' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9'] \n",
      "Val: Len: 4558, Unique: 8, People: ['imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15' 'imu\\\\2'\n",
      " 'imu\\\\3' 'imu\\\\5' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9'] \n",
      "Test: Len: 27337 , Unique: 8, People: ['imu\\\\1' 'imu\\\\4' 'imu\\\\6']\n",
      "-------------------------\n",
      "\n",
      "Fold 1, Train Subjects: ['imu\\\\1', 'imu\\\\10', 'imu\\\\11', 'imu\\\\12', 'imu\\\\13', 'imu\\\\15', 'imu\\\\2', 'imu\\\\4', 'imu\\\\5', 'imu\\\\6', 'imu\\\\7', 'imu\\\\9'], Test Subjects: ['imu\\\\14', 'imu\\\\3', 'imu\\\\8']\n",
      "Total Data: Seen: 74798, Unseen: 30398\n",
      "Inital\n",
      "Train: Len: 53854 , Unique: 6 People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\15' 'imu\\\\2'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\9']\n",
      "Val: Len: 5984, Unique: 6 People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\15' 'imu\\\\2'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\9']\n",
      "Test: Len: 19611, Unique: 6 People: ['imu\\\\14' 'imu\\\\3' 'imu\\\\8']\n",
      "Incremantal\n",
      "Train: Len: 40822 , Unique: 8, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\15' 'imu\\\\2'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\9'] \n",
      "Val: Len: 4536, Unique: 8, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\15' 'imu\\\\2'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\9'] \n",
      "Test: Len: 27353 , Unique: 8, People: ['imu\\\\14' 'imu\\\\3' 'imu\\\\8']\n",
      "-------------------------\n",
      "\n",
      "Fold 2, Train Subjects: ['imu\\\\1', 'imu\\\\12', 'imu\\\\13', 'imu\\\\14', 'imu\\\\15', 'imu\\\\2', 'imu\\\\3', 'imu\\\\4', 'imu\\\\5', 'imu\\\\6', 'imu\\\\7', 'imu\\\\8'], Test Subjects: ['imu\\\\10', 'imu\\\\11', 'imu\\\\9']\n",
      "Total Data: Seen: 75960, Unseen: 30475\n",
      "Inital\n",
      "Train: Len: 54691 , Unique: 6 People: ['imu\\\\1' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15' 'imu\\\\2' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8']\n",
      "Val: Len: 6077, Unique: 6 People: ['imu\\\\1' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15' 'imu\\\\2' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8']\n",
      "Test: Len: 18449, Unique: 6 People: ['imu\\\\10' 'imu\\\\11' 'imu\\\\9']\n",
      "Incremantal\n",
      "Train: Len: 41099 , Unique: 8, People: ['imu\\\\1' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15' 'imu\\\\2' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8'] \n",
      "Val: Len: 4568, Unique: 8, People: ['imu\\\\1' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15' 'imu\\\\2' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8'] \n",
      "Test: Len: 26114 , Unique: 8, People: ['imu\\\\10' 'imu\\\\11' 'imu\\\\9']\n",
      "-------------------------\n",
      "\n",
      "Fold 3, Train Subjects: ['imu\\\\1', 'imu\\\\10', 'imu\\\\11', 'imu\\\\12', 'imu\\\\14', 'imu\\\\15', 'imu\\\\3', 'imu\\\\4', 'imu\\\\6', 'imu\\\\7', 'imu\\\\8', 'imu\\\\9'], Test Subjects: ['imu\\\\13', 'imu\\\\2', 'imu\\\\5']\n",
      "Total Data: Seen: 76830, Unseen: 30477\n",
      "Inital\n",
      "Train: Len: 55317 , Unique: 6 People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\14' 'imu\\\\15' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9']\n",
      "Val: Len: 6147, Unique: 6 People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\14' 'imu\\\\15' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9']\n",
      "Test: Len: 17579, Unique: 6 People: ['imu\\\\13' 'imu\\\\2' 'imu\\\\5']\n",
      "Incremantal\n",
      "Train: Len: 41258 , Unique: 8, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\14' 'imu\\\\15' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9'] \n",
      "Val: Len: 4585, Unique: 8, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\14' 'imu\\\\15' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9'] \n",
      "Test: Len: 25242 , Unique: 8, People: ['imu\\\\13' 'imu\\\\2' 'imu\\\\5']\n",
      "-------------------------\n",
      "\n",
      "Fold 4, Train Subjects: ['imu\\\\1', 'imu\\\\10', 'imu\\\\11', 'imu\\\\13', 'imu\\\\14', 'imu\\\\2', 'imu\\\\3', 'imu\\\\4', 'imu\\\\5', 'imu\\\\6', 'imu\\\\8', 'imu\\\\9'], Test Subjects: ['imu\\\\12', 'imu\\\\15', 'imu\\\\7']\n",
      "Total Data: Seen: 75493, Unseen: 30553\n",
      "Inital\n",
      "Train: Len: 54354 , Unique: 6 People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\13' 'imu\\\\14' 'imu\\\\2' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\8' 'imu\\\\9']\n",
      "Val: Len: 6040, Unique: 6 People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\13' 'imu\\\\14' 'imu\\\\2' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\8' 'imu\\\\9']\n",
      "Test: Len: 18916, Unique: 6 People: ['imu\\\\12' 'imu\\\\15' 'imu\\\\7']\n",
      "Incremantal\n",
      "Train: Len: 41086 , Unique: 8, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\13' 'imu\\\\14' 'imu\\\\2' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\8' 'imu\\\\9'] \n",
      "Val: Len: 4566, Unique: 8, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\13' 'imu\\\\14' 'imu\\\\2' 'imu\\\\3'\n",
      " 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\8' 'imu\\\\9'] \n",
      "Test: Len: 26503 , Unique: 8, People: ['imu\\\\12' 'imu\\\\15' 'imu\\\\7']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = 'realworld'\n",
    "\n",
    "standardize = True\n",
    "if(standardize): \n",
    "        name_add = \"_Standardized\"\n",
    "else: \n",
    "        name_add =\"\"\n",
    "if(dataset == 'mhealth'):\n",
    "    # data_x = np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\X_standarized.npy\")\n",
    "    # data_y= np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\Y_standarized.npy\")\n",
    "    # data_p = np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\pid_standarized.npy\")\n",
    "    \n",
    "    \n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\mhealth+dataset\\data\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y= np.load(fr\"{os.getcwd()}\\Datasets\\mhealth+dataset\\data\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\mhealth+dataset\\data\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    # data_time = np.load(r\"C:\\Users\\Dhruv\\Downloads\\Compressed\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\time.npy\")\n",
    "    # scenario1_step_subjects = [personid for personid in np.unique(data_p) if str(personid) not in ['1', '1618', '1637', '1639', '1642']]\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "    \n",
    "    initial_step_classes = np.unique(data_y)[0:6]\n",
    "    incremental_step_classes = np.unique(data_y)[6:]\n",
    "\n",
    "if(dataset == 'wisdm'):\n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y= np.load(fr\"{os.getcwd()}\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    # data_time = np.load(r\"C:\\Users\\Dhruv\\Downloads\\Compressed\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\time.npy\")\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p) if str(personid) not in ['1616', '1618', '1637', '1639', '1642']]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "    \n",
    "    initial_step_classes = np.unique(data_y)[0:14]\n",
    "    incremental_step_classes = np.unique(data_y)[14:]\n",
    "if(dataset == 'realworld'):\n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\RealWorld\\realworld_30hz_w10\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y = np.load(fr\"{os.getcwd()}\\Datasets\\RealWorld\\realworld_30hz_w10\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\RealWorld\\realworld_30hz_w10\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "\n",
    "     \n",
    "\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "    initial_step_classes = np.unique(data_y)[0:6]\n",
    "    incremental_step_classes = np.unique(data_y)[6:]\n",
    "    # initial_step_classes = ['climbingup','climbingdown',\n",
    "    #                         'jumping','running','walking']\n",
    "    # incremental_step_classes = ['lying', 'sitting', 'standing']\n",
    "\n",
    "\n",
    "# if(dataset == 'oppo'):\n",
    "#     data_x = np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\X.npy\")\n",
    "#     data_y= np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\Y.npy\")\n",
    "#     data_p = np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\pid.npy\")\n",
    "#     scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "#     incremental_step_classes = ['406520.0', '404505.0', '404508.0', '404511.0']\n",
    "#     initial_step_classes = [y for y in np.unique(data_y) if y not in  incremental_step_classes]\n",
    "    \n",
    "\n",
    "if(dataset == 'pamap'):\n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y= np.load(fr\"{os.getcwd()}\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]  # all the subjects\n",
    "    \n",
    "    incremental_step_classes = [10 ,18 ,19, 20, 11, 9] # Classes in Incremental Steps\n",
    "    initial_step_classes = np.array([y for y in np.unique(data_y) if y not in  incremental_step_classes])\n",
    "    \n",
    "    print(\"Classes before transforming\", np.unique(data_y))\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = np.array(le.transform(data_y))\n",
    "    \n",
    "    \n",
    "    print(initial_step_classes,incremental_step_classes )\n",
    "    initial_step_classes = le.transform(initial_step_classes)\n",
    "    incremental_step_classes = le.transform(incremental_step_classes)\n",
    "\n",
    "    print(\"Classes After transforming\", np.unique(data_y))\n",
    "    print(initial_step_classes,incremental_step_classes )\n",
    "    classes = np.concatenate([initial_step_classes , incremental_step_classes])\n",
    "\n",
    "    print(\"Classes after ordering the transformed classes\")\n",
    "    changed_data_y = []  # Changing the order of the classes\n",
    "    for i,clas in enumerate(classes):\n",
    "        data_y[data_y == clas] = -i\n",
    "    data_y = -data_y\n",
    "    \n",
    "    initial_step_classes =np.unique(data_y)[:len(initial_step_classes)]\n",
    "    incremental_step_classes = np.unique(data_y)[len(initial_step_classes):]\n",
    "    \n",
    "\n",
    "# Scenario 1\n",
    "# Here Scenario 1 is where we use same test data for both initial and incremental step.\n",
    "\n",
    "scenario =1\n",
    "print(\"Dataset :\", dataset)\n",
    "print(\"Number of Classes: {}\\nAll Classes:  \\t{}\".format(len(np.unique(data_y)), np.unique(data_y)))\n",
    "print(\"\\nInital Step Classes: {}\\nIncremental Step New Classes: {}\".format(initial_step_classes, incremental_step_classes))\n",
    "print('Subjects: ',scenario1_step_subjects)\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(np.unique(data_p))):\n",
    "    data_seen_classes_scenario_1_x,data_seen_classes_scenario_1_y, data_seen_classes_scenario_1_p = [], [], []\n",
    "    data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p = [], [], []\n",
    "    data_test_scenario_1_x, data_test_scenario_1_y,data_test_scenario_1_p = [], [], []\n",
    "    data_incremental_step_x_test, data_incremental_step_y_test, data_incremental_step_p_test = [], [], []\n",
    "    data_initial_step_x_test, data_initial_step_y_test, data_initial_step_p_test = [], [], []\n",
    "    \n",
    "    train_subjects_in_fold = np.unique(data_p)[train_index].tolist()\n",
    "    test_subjects_in_fold = np.unique(data_p)[test_index].tolist()\n",
    "    \n",
    "    # print(type(fold), type(list(train_subjects_in_fold.tolist())[0]))\n",
    "    print(\"-------------------------\\n\")\n",
    "    print(f\"Fold {fold}, Train Subjects: {train_subjects_in_fold}, Test Subjects: {test_subjects_in_fold}\")\n",
    "    for index,person in enumerate(data_p):\n",
    "        # print(person, train_subjects_in_fold, data_y[index], initial_step_classes)\n",
    "        \n",
    "        if(person in train_subjects_in_fold and data_y[index] in initial_step_classes):\n",
    "            \n",
    "            data_seen_classes_scenario_1_x.append(data_x[index])\n",
    "            data_seen_classes_scenario_1_y.append(data_y[index])\n",
    "            data_seen_classes_scenario_1_p.append(data_p[index])\n",
    "\n",
    "        if(person in train_subjects_in_fold and data_y[index] in incremental_step_classes):\n",
    "            \n",
    "            data_unseen_classes_scenario_1_x.append(data_x[index])\n",
    "            data_unseen_classes_scenario_1_y.append(data_y[index])\n",
    "            data_unseen_classes_scenario_1_p.append(data_p[index])\n",
    "        if(person in test_subjects_in_fold and data_y[index] in initial_step_classes):\n",
    "            data_initial_step_x_test.append(data_x[index])\n",
    "            data_initial_step_y_test.append(data_y[index])\n",
    "            data_initial_step_p_test.append(data_p[index])\n",
    "\n",
    "        if(person in test_subjects_in_fold):\n",
    "            \n",
    "            data_incremental_step_x_test.append(data_x[index])\n",
    "            data_incremental_step_y_test.append(data_y[index])\n",
    "            data_incremental_step_p_test.append(data_p[index])\n",
    "\n",
    "        if(((person in train_subjects_in_fold and data_y[index] in initial_step_classes) or (person in train_subjects_in_fold and data_y[index] in incremental_step_classes)\n",
    "            or (person in test_subjects_in_fold and data_y[index] in initial_step_classes) or (person in test_subjects_in_fold)) == False):\n",
    "            print(\"Some Issue\")   \n",
    "        # else:\n",
    "           \n",
    "        #     print(\"Some thing is left..\") # Just to check if any element falls outside our conditions.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Seen Classes split for Initial and Incremental Step\n",
    "    data_initial_step_scenario_1_x_seen, data_incremental_step_scenario_1_x_seen, data_initial_step_scenario_1_y_seen, data_incremental_step_scenario_1_y_seen, data_initial_step_scenario_1_p_seen, data_incremental_step_scenario_1_p_seen = train_test_split(data_seen_classes_scenario_1_x, data_seen_classes_scenario_1_y, data_seen_classes_scenario_1_p, test_size=0.20, stratify=(data_seen_classes_scenario_1_y), random_state=42)\n",
    "\n",
    "    # Seen Classes Train and validation Split for Inital Step. \n",
    "    data_initial_step_scenario_1_x_train, data_initial_step_scenario_1_x_val, data_initial_step_scenario_1_y_train, data_initial_step_scenario_1_y_val, data_initial_step_scenario_1_p_train, data_initial_step_scenario_1_p_val = train_test_split(data_initial_step_scenario_1_x_seen, data_initial_step_scenario_1_y_seen,data_initial_step_scenario_1_p_seen, test_size=0.10, stratify=(data_initial_step_scenario_1_y_seen), random_state=42)\n",
    "    \n",
    "    # Seen Classes Train and validation Split for Incremental Step. \n",
    "    data_incremental_step_scenario_1_x_train, data_incremental_step_scenario_1_x_val, data_incremental_step_scenario_1_y_train, data_incremental_step_scenario_1_y_val, data_incremental_step_scenario_1_p_train, data_incremental_step_scenario_1_p_val = train_test_split(data_incremental_step_scenario_1_x_seen, data_incremental_step_scenario_1_y_seen,data_incremental_step_scenario_1_p_seen, test_size=0.1, stratify=(data_incremental_step_scenario_1_y_seen), random_state=42)\n",
    "\n",
    "    # UnSeen Classes Train and validation Split for Incremental Step. \n",
    "    data_incremental_step_scenario_1_x_train_unseen, data_incremental_step_scenario_1_x_val_unseen, data_incremental_step_scenario_1_y_train_unseen, data_incremental_step_scenario_1_y_val_unseen, data_incremental_step_scenario_1_p_train_unseen, data_incremental_step_scenario_1_p_val_unseen = train_test_split(data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p, test_size=0.1, stratify=(data_unseen_classes_scenario_1_y), random_state=42)\n",
    "\n",
    "    # Combining seen and unseen classes for Incremental Classes\n",
    "    data_incremental_step_scenario_1_x_train = np.concatenate((data_incremental_step_scenario_1_x_train, data_incremental_step_scenario_1_x_train_unseen))\n",
    "    data_incremental_step_scenario_1_y_train = np.concatenate((data_incremental_step_scenario_1_y_train, data_incremental_step_scenario_1_y_train_unseen))\n",
    "    data_incremental_step_scenario_1_p_train = np.concatenate((data_incremental_step_scenario_1_p_train, data_incremental_step_scenario_1_p_train_unseen))\n",
    "\n",
    "    data_incremental_step_scenario_1_x_val = np.concatenate((data_incremental_step_scenario_1_x_val, data_incremental_step_scenario_1_x_val_unseen))\n",
    "    data_incremental_step_scenario_1_y_val = np.concatenate((data_incremental_step_scenario_1_y_val, data_incremental_step_scenario_1_y_val_unseen))\n",
    "    data_incremental_step_scenario_1_p_val = np.concatenate((data_incremental_step_scenario_1_p_val, data_incremental_step_scenario_1_p_val_unseen))\n",
    "\n",
    "    print(f\"Total Data: Seen: {len(data_seen_classes_scenario_1_x)}, Unseen: {len(data_unseen_classes_scenario_1_x)}\")\n",
    "    print(\"Inital\")\n",
    "    print(f\"Train: Len: {len(data_initial_step_scenario_1_x_train)} , Unique: {len(np.unique(data_initial_step_scenario_1_y_train))} People: {np.unique(data_initial_step_scenario_1_p_train)}\")\n",
    "    print(f\"Val: Len: {len(data_initial_step_scenario_1_x_val)}, Unique: {len(np.unique(data_initial_step_scenario_1_y_val))} People: {np.unique(data_initial_step_scenario_1_p_val)}\") \n",
    "    print(f\"Test: Len: {len(data_initial_step_x_test)}, Unique: {len(np.unique(data_initial_step_y_test))} People: {np.unique(data_initial_step_p_test)}\")\n",
    "\n",
    "    print(\"Incremantal\")\n",
    "    \n",
    "    print(f\"Train: Len: {len(data_incremental_step_scenario_1_x_train)} , Unique: {len(np.unique(data_incremental_step_scenario_1_y_train))}, People: {np.unique(data_incremental_step_scenario_1_p_train)} \")\n",
    "    print(f\"Val: Len: {len(data_incremental_step_scenario_1_x_val)}, Unique: {len(np.unique(data_incremental_step_scenario_1_y_val))}, People: {np.unique(data_incremental_step_scenario_1_p_val)} \")    \n",
    "    print(f\"Test: Len: {len(data_incremental_step_x_test)} , Unique: {len(np.unique(data_incremental_step_y_test))}, People: {np.unique(data_incremental_step_p_test)}\")\n",
    "    \n",
    "\n",
    "    # Saving the necessary files\n",
    "    windowlen = np.array(data_incremental_step_scenario_1_x_train).shape[1]\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_x_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_y_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_p_train)\n",
    "\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_x_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_y_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_p_val)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_x_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_y_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_p_test)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_x_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_y_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_p_train)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_x_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_y_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_p_val)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_x_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_y_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_p_test)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n",
    "\n",
    "print(le.fit_transform(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"personid\":data_p, \"activityid\":data_y})\n",
    "activities_by_person = df.groupby('personid')['activityid'].nunique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personid</th>\n",
       "      <th>activityid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  personid  activityid\n",
       "0        1          12\n",
       "1       10          12\n",
       "2        2          12\n",
       "3        3          12\n",
       "4        4          12\n",
       "5        5          12\n",
       "6        6          12\n",
       "7        7          12\n",
       "8        8          12\n",
       "9        9          12"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities_by_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27337"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.load(r'C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\HAR_data\\realworld\\data_incremental_step_scenario_1_p_test_windowLen_100_standardized_True_fold0.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
