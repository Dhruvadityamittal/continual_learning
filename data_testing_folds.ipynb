{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : wisdm\n",
      "Number of Classes: 18\n",
      "All Classes:  \t[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n",
      "\n",
      "Inital Step Classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "Incremental Step New Classes: [14 15 16 17]\n",
      "Subjects:  [1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1617, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1638, 1640, 1641, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650]\n",
      "-------------------------\n",
      "\n",
      "Fold 0, Train Subjects: [1600, 1601, 1602, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1614, 1615, 1616, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1626, 1627, 1628, 1629, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1641, 1642, 1644, 1645, 1647, 1648, 1649, 1650], Test Subjects: [1603, 1612, 1613, 1617, 1624, 1630, 1631, 1632, 1640, 1643, 1646]\n",
      "Total Data: Seen: 65763, Unseen: 18879\n",
      "Inital\n",
      "Train: Len: 47349 , Unique: 14 People: [1600 1601 1602 1604 1605 1606 1607 1608 1609 1610 1611 1614 1615 1616\n",
      " 1618 1619 1620 1621 1622 1623 1625 1626 1627 1628 1629 1633 1634 1635\n",
      " 1636 1637 1638 1639 1641 1642 1644 1645 1647 1648 1649 1650]\n",
      "Val: Len: 5261, Unique: 14 People: [1600 1601 1602 1604 1605 1606 1607 1608 1609 1610 1611 1614 1615 1616\n",
      " 1618 1619 1620 1621 1622 1623 1625 1626 1627 1628 1629 1633 1634 1635\n",
      " 1636 1637 1638 1639 1641 1642 1644 1645 1647 1648 1649 1650]\n",
      "Test: Len: 18254, Unique: 14 People: [1603 1612 1613 1617 1624 1630 1631 1632 1640 1643 1646]\n",
      "Incremantal\n",
      "Train: Len: 28828 , Unique: 18, People: [1600 1601 1602 1604 1605 1606 1607 1608 1609 1610 1611 1614 1615 1616\n",
      " 1618 1619 1620 1621 1622 1623 1625 1626 1627 1628 1629 1633 1634 1635\n",
      " 1636 1637 1638 1639 1641 1642 1644 1645 1647 1648 1649 1650] \n",
      "Val: Len: 3204, Unique: 18, People: [1600 1601 1602 1604 1605 1606 1607 1608 1609 1610 1611 1614 1615 1616\n",
      " 1618 1619 1620 1621 1622 1623 1625 1626 1627 1628 1629 1633 1634 1635\n",
      " 1636 1637 1638 1639 1641 1642 1644 1645 1647 1648 1649 1650] \n",
      "Test: Len: 23479 , Unique: 18, People: [1603 1612 1613 1617 1624 1630 1631 1632 1640 1643 1646]\n",
      "-------------------------\n",
      "\n",
      "Fold 1, Train Subjects: [1600, 1601, 1602, 1603, 1605, 1607, 1609, 1610, 1611, 1612, 1613, 1614, 1616, 1617, 1618, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1634, 1635, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1650], Test Subjects: [1604, 1606, 1608, 1615, 1619, 1633, 1636, 1647, 1648, 1649]\n",
      "Total Data: Seen: 67404, Unseen: 19354\n",
      "Inital\n",
      "Train: Len: 48530 , Unique: 14 People: [1600 1601 1602 1603 1605 1607 1609 1610 1611 1612 1613 1614 1616 1617\n",
      " 1618 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632\n",
      " 1634 1635 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1650]\n",
      "Val: Len: 5393, Unique: 14 People: [1600 1601 1602 1603 1605 1607 1609 1610 1611 1612 1613 1614 1616 1617\n",
      " 1618 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632\n",
      " 1634 1635 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1650]\n",
      "Test: Len: 16613, Unique: 14 People: [1604 1606 1608 1615 1619 1633 1636 1647 1648 1649]\n",
      "Incremantal\n",
      "Train: Len: 29550 , Unique: 18, People: [1600 1601 1602 1603 1605 1607 1609 1610 1611 1612 1613 1614 1616 1617\n",
      " 1618 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632\n",
      " 1634 1635 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1650] \n",
      "Val: Len: 3285, Unique: 18, People: [1600 1601 1602 1603 1605 1607 1609 1610 1611 1612 1613 1614 1616 1617\n",
      " 1618 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632\n",
      " 1634 1635 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1650] \n",
      "Test: Len: 21363 , Unique: 18, People: [1604 1606 1608 1615 1619 1633 1636 1647 1648 1649]\n",
      "-------------------------\n",
      "\n",
      "Fold 2, Train Subjects: [1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1610, 1612, 1613, 1614, 1615, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1628, 1629, 1630, 1631, 1632, 1633, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1646, 1647, 1648, 1649, 1650], Test Subjects: [1600, 1609, 1611, 1616, 1625, 1626, 1627, 1634, 1644, 1645]\n",
      "Total Data: Seen: 67634, Unseen: 19344\n",
      "Inital\n",
      "Train: Len: 48696 , Unique: 14 People: [1601 1602 1603 1604 1605 1606 1607 1608 1610 1612 1613 1614 1615 1617\n",
      " 1618 1619 1620 1621 1622 1623 1624 1628 1629 1630 1631 1632 1633 1635\n",
      " 1636 1637 1638 1639 1640 1641 1642 1643 1646 1647 1648 1649 1650]\n",
      "Val: Len: 5411, Unique: 14 People: [1601 1602 1603 1604 1605 1606 1607 1608 1610 1612 1613 1614 1615 1617\n",
      " 1618 1619 1620 1621 1622 1623 1624 1628 1629 1630 1631 1632 1633 1635\n",
      " 1636 1637 1638 1639 1640 1641 1642 1643 1646 1647 1648 1649 1650]\n",
      "Test: Len: 16383, Unique: 14 People: [1600 1609 1611 1616 1625 1626 1627 1634 1644 1645]\n",
      "Incremantal\n",
      "Train: Len: 29583 , Unique: 18, People: [1601 1602 1603 1604 1605 1606 1607 1608 1610 1612 1613 1614 1615 1617\n",
      " 1618 1619 1620 1621 1622 1623 1624 1628 1629 1630 1631 1632 1633 1635\n",
      " 1636 1637 1638 1639 1640 1641 1642 1643 1646 1647 1648 1649 1650] \n",
      "Val: Len: 3288, Unique: 18, People: [1601 1602 1603 1604 1605 1606 1607 1608 1610 1612 1613 1614 1615 1617\n",
      " 1618 1619 1620 1621 1622 1623 1624 1628 1629 1630 1631 1632 1633 1635\n",
      " 1636 1637 1638 1639 1640 1641 1642 1643 1646 1647 1648 1649 1650] \n",
      "Test: Len: 21143 , Unique: 18, People: [1600 1609 1611 1616 1625 1626 1627 1634 1644 1645]\n",
      "-------------------------\n",
      "\n",
      "Fold 3, Train Subjects: [1600, 1603, 1604, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1622, 1624, 1625, 1626, 1627, 1628, 1630, 1631, 1632, 1633, 1634, 1636, 1638, 1640, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650], Test Subjects: [1601, 1602, 1605, 1621, 1623, 1629, 1635, 1637, 1639, 1641]\n",
      "Total Data: Seen: 67631, Unseen: 19346\n",
      "Inital\n",
      "Train: Len: 48693 , Unique: 14 People: [1600 1603 1604 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616\n",
      " 1617 1618 1619 1620 1622 1624 1625 1626 1627 1628 1630 1631 1632 1633\n",
      " 1634 1636 1638 1640 1642 1643 1644 1645 1646 1647 1648 1649 1650]\n",
      "Val: Len: 5411, Unique: 14 People: [1600 1603 1604 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616\n",
      " 1617 1618 1619 1620 1622 1624 1625 1626 1627 1628 1630 1631 1632 1633\n",
      " 1634 1636 1638 1640 1642 1643 1644 1645 1646 1647 1648 1649 1650]\n",
      "Test: Len: 16386, Unique: 14 People: [1601 1602 1605 1621 1623 1629 1635 1637 1639 1641]\n",
      "Incremantal\n",
      "Train: Len: 29585 , Unique: 18, People: [1600 1603 1604 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616\n",
      " 1617 1618 1619 1620 1622 1624 1625 1626 1627 1628 1630 1631 1632 1633\n",
      " 1634 1636 1638 1640 1642 1643 1644 1645 1646 1647 1648 1649 1650] \n",
      "Val: Len: 3288, Unique: 18, People: [1600 1603 1604 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616\n",
      " 1617 1618 1619 1620 1622 1624 1625 1626 1627 1628 1630 1631 1632 1633\n",
      " 1634 1636 1638 1640 1642 1643 1644 1645 1646 1647 1648 1649 1650] \n",
      "Test: Len: 21144 , Unique: 18, People: [1601 1602 1605 1621 1623 1629 1635 1637 1639 1641]\n",
      "-------------------------\n",
      "\n",
      "Fold 4, Train Subjects: [1600, 1601, 1602, 1603, 1604, 1605, 1606, 1608, 1609, 1611, 1612, 1613, 1615, 1616, 1617, 1619, 1621, 1623, 1624, 1625, 1626, 1627, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1639, 1640, 1641, 1643, 1644, 1645, 1646, 1647, 1648, 1649], Test Subjects: [1607, 1610, 1614, 1618, 1620, 1622, 1628, 1638, 1642, 1650]\n",
      "Total Data: Seen: 67636, Unseen: 19493\n",
      "Inital\n",
      "Train: Len: 48697 , Unique: 14 People: [1600 1601 1602 1603 1604 1605 1606 1608 1609 1611 1612 1613 1615 1616\n",
      " 1617 1619 1621 1623 1624 1625 1626 1627 1629 1630 1631 1632 1633 1634\n",
      " 1635 1636 1637 1639 1640 1641 1643 1644 1645 1646 1647 1648 1649]\n",
      "Val: Len: 5411, Unique: 14 People: [1600 1601 1602 1603 1604 1605 1606 1608 1609 1611 1612 1613 1615 1616\n",
      " 1617 1619 1621 1623 1624 1625 1626 1627 1629 1630 1631 1632 1633 1634\n",
      " 1635 1636 1637 1639 1640 1641 1643 1644 1645 1646 1647 1648 1649]\n",
      "Test: Len: 16381, Unique: 14 People: [1607 1610 1614 1618 1620 1622 1628 1638 1642 1650]\n",
      "Incremantal\n",
      "Train: Len: 29718 , Unique: 18, People: [1600 1601 1602 1603 1604 1605 1606 1608 1609 1611 1612 1613 1615 1616\n",
      " 1617 1619 1621 1623 1624 1625 1626 1627 1629 1630 1631 1632 1633 1634\n",
      " 1635 1636 1637 1639 1640 1641 1643 1644 1645 1646 1647 1648 1649] \n",
      "Val: Len: 3303, Unique: 18, People: [1600 1601 1602 1603 1604 1605 1606 1608 1609 1611 1612 1613 1615 1616\n",
      " 1617 1619 1621 1623 1624 1625 1626 1627 1629 1630 1631 1632 1633 1634\n",
      " 1635 1636 1637 1639 1640 1641 1643 1644 1645 1646 1647 1648 1649] \n",
      "Test: Len: 20992 , Unique: 18, People: [1607 1610 1614 1618 1620 1622 1628 1638 1642 1650]\n"
     ]
    }
   ],
   "source": [
    "# Wisdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = 'wisdm'\n",
    "# Wisdm\n",
    "standardize = True\n",
    "if(standardize): \n",
    "        name_add = \"_Standardized\"\n",
    "else: \n",
    "        name_add =\"\"\n",
    "if(dataset == 'mhealth'):\n",
    "    # data_x = np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\X_standarized.npy\")\n",
    "    # data_y= np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\Y_standarized.npy\")\n",
    "    # data_p = np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\pid_standarized.npy\")\n",
    "    \n",
    "    \n",
    "    data_x = np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\mhealth+dataset\\data\\X{name_add}.npy\")\n",
    "    data_y= np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\mhealth+dataset\\data\\Y{name_add}.npy\")\n",
    "    data_p = np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\mhealth+dataset\\data\\pid{name_add}.npy\")\n",
    "    # data_time = np.load(r\"C:\\Users\\Dhruv\\Downloads\\Compressed\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\time.npy\")\n",
    "    # scenario1_step_subjects = [personid for personid in np.unique(data_p) if str(personid) not in ['1', '1618', '1637', '1639', '1642']]\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "    \n",
    "    initial_step_classes = np.unique(data_y)[0:6]\n",
    "    incremental_step_classes = np.unique(data_y)[6:]\n",
    "\n",
    "if(dataset == 'wisdm'):\n",
    "    data_x = np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\X{name_add}.npy\")\n",
    "    data_y= np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\Y{name_add}.npy\")\n",
    "    data_p = np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\pid{name_add}.npy\")\n",
    "    # data_time = np.load(r\"C:\\Users\\Dhruv\\Downloads\\Compressed\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\time.npy\")\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p) if str(personid) not in ['1616', '1618', '1637', '1639', '1642']]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "    \n",
    "    initial_step_classes = np.unique(data_y)[0:14]\n",
    "    incremental_step_classes = np.unique(data_y)[14:]\n",
    "if(dataset == 'realworld'):\n",
    "    data_x = np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\RealWorld\\realworld_30hz_w10\\X{name_add}.npy\")\n",
    "    data_y = np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\RealWorld\\realworld_30hz_w10\\Y{name_add}.npy\")\n",
    "    data_p = np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\RealWorld\\realworld_30hz_w10\\pid{name_add}.npy\")\n",
    "    \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "\n",
    "     \n",
    "\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "    initial_step_classes = np.unique(data_y)[0:6]\n",
    "    incremental_step_classes = np.unique(data_y)[6:]\n",
    "    # initial_step_classes = ['climbingup','climbingdown',\n",
    "    #                         'jumping','running','walking']\n",
    "    # incremental_step_classes = ['lying', 'sitting', 'standing']\n",
    "\n",
    "\n",
    "if(dataset == 'oppo'):\n",
    "    data_x = np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\X.npy\")\n",
    "    data_y= np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\Y.npy\")\n",
    "    data_p = np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\pid.npy\")\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "    incremental_step_classes = ['406520.0', '404505.0', '404508.0', '404511.0']\n",
    "    initial_step_classes = [y for y in np.unique(data_y) if y not in  incremental_step_classes]\n",
    "    \n",
    "\n",
    "if(dataset == 'pamap'):\n",
    "    data_x = np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\X{name_add}.npy\")\n",
    "    data_y= np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\Y{name_add}.npy\")\n",
    "    data_p = np.load(fr\"C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\pid{name_add}.npy\")\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]  # all the subjects\n",
    "    \n",
    "    incremental_step_classes = [10 ,18 ,19, 20, 11, 9] # Classes in Incremental Steps\n",
    "    initial_step_classes = np.array([y for y in np.unique(data_y) if y not in  incremental_step_classes])\n",
    "    \n",
    "    print(\"Classes before transforming\", np.unique(data_y))\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = np.array(le.transform(data_y))\n",
    "    \n",
    "    \n",
    "    print(initial_step_classes,incremental_step_classes )\n",
    "    initial_step_classes = le.transform(initial_step_classes)\n",
    "    incremental_step_classes = le.transform(incremental_step_classes)\n",
    "\n",
    "    print(\"Classes After transforming\", np.unique(data_y))\n",
    "    print(initial_step_classes,incremental_step_classes )\n",
    "    classes = np.concatenate([initial_step_classes , incremental_step_classes])\n",
    "\n",
    "    print(\"Classes after ordering the transformed classes\")\n",
    "    changed_data_y = []  # Changing the order of the classes\n",
    "    for i,clas in enumerate(classes):\n",
    "        data_y[data_y == clas] = -i\n",
    "    data_y = -data_y\n",
    "    \n",
    "    initial_step_classes =np.unique(data_y)[:len(initial_step_classes)]\n",
    "    incremental_step_classes = np.unique(data_y)[len(initial_step_classes):]\n",
    "    \n",
    "\n",
    "# Scenario 1\n",
    "# Here Scenario 1 is where we use same test data for both initial and incremental step.\n",
    "\n",
    "scenario =1\n",
    "print(\"Dataset :\", dataset)\n",
    "print(\"Number of Classes: {}\\nAll Classes:  \\t{}\".format(len(np.unique(data_y)), np.unique(data_y)))\n",
    "print(\"\\nInital Step Classes: {}\\nIncremental Step New Classes: {}\".format(initial_step_classes, incremental_step_classes))\n",
    "print('Subjects: ',scenario1_step_subjects)\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(np.unique(data_p))):\n",
    "    data_seen_classes_scenario_1_x,data_seen_classes_scenario_1_y, data_seen_classes_scenario_1_p = [], [], []\n",
    "    data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p = [], [], []\n",
    "    data_test_scenario_1_x, data_test_scenario_1_y,data_test_scenario_1_p = [], [], []\n",
    "    data_incremental_step_x_test, data_incremental_step_y_test, data_incremental_step_p_test = [], [], []\n",
    "    data_initial_step_x_test, data_initial_step_y_test, data_initial_step_p_test = [], [], []\n",
    "    \n",
    "    train_subjects_in_fold = np.unique(data_p)[train_index].tolist()\n",
    "    test_subjects_in_fold = np.unique(data_p)[test_index].tolist()\n",
    "    \n",
    "    # print(type(fold), type(list(train_subjects_in_fold.tolist())[0]))\n",
    "    print(\"-------------------------\\n\")\n",
    "    print(f\"Fold {fold}, Train Subjects: {train_subjects_in_fold}, Test Subjects: {test_subjects_in_fold}\")\n",
    "    for index,person in enumerate(data_p):\n",
    "        # print(person, train_subjects_in_fold, data_y[index], initial_step_classes)\n",
    "        \n",
    "        if(person in train_subjects_in_fold and data_y[index] in initial_step_classes):\n",
    "            \n",
    "            data_seen_classes_scenario_1_x.append(data_x[index])\n",
    "            data_seen_classes_scenario_1_y.append(data_y[index])\n",
    "            data_seen_classes_scenario_1_p.append(data_p[index])\n",
    "\n",
    "        if(person in train_subjects_in_fold and data_y[index] in incremental_step_classes):\n",
    "            \n",
    "            data_unseen_classes_scenario_1_x.append(data_x[index])\n",
    "            data_unseen_classes_scenario_1_y.append(data_y[index])\n",
    "            data_unseen_classes_scenario_1_p.append(data_p[index])\n",
    "        if(person in test_subjects_in_fold and data_y[index] in initial_step_classes):\n",
    "            data_initial_step_x_test.append(data_x[index])\n",
    "            data_initial_step_y_test.append(data_y[index])\n",
    "            data_initial_step_p_test.append(data_p[index])\n",
    "\n",
    "        if(person in test_subjects_in_fold):\n",
    "            \n",
    "            data_incremental_step_x_test.append(data_x[index])\n",
    "            data_incremental_step_y_test.append(data_y[index])\n",
    "            data_incremental_step_p_test.append(data_p[index])\n",
    "\n",
    "        if(((person in train_subjects_in_fold and data_y[index] in initial_step_classes) or (person in train_subjects_in_fold and data_y[index] in incremental_step_classes)\n",
    "            or (person in test_subjects_in_fold and data_y[index] in initial_step_classes) or (person in test_subjects_in_fold)) == False):\n",
    "            print(\"Some Issue\")   \n",
    "        # else:\n",
    "           \n",
    "        #     print(\"Some thing is left..\") # Just to check if any element falls outside our conditions.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Seen Classes split for Initial and Incremental Step\n",
    "    data_initial_step_scenario_1_x_seen, data_incremental_step_scenario_1_x_seen, data_initial_step_scenario_1_y_seen, data_incremental_step_scenario_1_y_seen, data_initial_step_scenario_1_p_seen, data_incremental_step_scenario_1_p_seen = train_test_split(data_seen_classes_scenario_1_x, data_seen_classes_scenario_1_y, data_seen_classes_scenario_1_p, test_size=0.20, stratify=(data_seen_classes_scenario_1_y), random_state=42)\n",
    "\n",
    "    # Seen Classes Train and validation Split for Inital Step. \n",
    "    data_initial_step_scenario_1_x_train, data_initial_step_scenario_1_x_val, data_initial_step_scenario_1_y_train, data_initial_step_scenario_1_y_val, data_initial_step_scenario_1_p_train, data_initial_step_scenario_1_p_val = train_test_split(data_initial_step_scenario_1_x_seen, data_initial_step_scenario_1_y_seen,data_initial_step_scenario_1_p_seen, test_size=0.10, stratify=(data_initial_step_scenario_1_y_seen), random_state=42)\n",
    "    \n",
    "    # Seen Classes Train and validation Split for Incremental Step. \n",
    "    data_incremental_step_scenario_1_x_train, data_incremental_step_scenario_1_x_val, data_incremental_step_scenario_1_y_train, data_incremental_step_scenario_1_y_val, data_incremental_step_scenario_1_p_train, data_incremental_step_scenario_1_p_val = train_test_split(data_incremental_step_scenario_1_x_seen, data_incremental_step_scenario_1_y_seen,data_incremental_step_scenario_1_p_seen, test_size=0.1, stratify=(data_incremental_step_scenario_1_y_seen), random_state=42)\n",
    "\n",
    "    # UnSeen Classes Train and validation Split for Incremental Step. \n",
    "    data_incremental_step_scenario_1_x_train_unseen, data_incremental_step_scenario_1_x_val_unseen, data_incremental_step_scenario_1_y_train_unseen, data_incremental_step_scenario_1_y_val_unseen, data_incremental_step_scenario_1_p_train_unseen, data_incremental_step_scenario_1_p_val_unseen = train_test_split(data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p, test_size=0.1, stratify=(data_unseen_classes_scenario_1_y), random_state=42)\n",
    "\n",
    "    # Combining seen and unseen classes for Incremental Classes\n",
    "    data_incremental_step_scenario_1_x_train = np.concatenate((data_incremental_step_scenario_1_x_train, data_incremental_step_scenario_1_x_train_unseen))\n",
    "    data_incremental_step_scenario_1_y_train = np.concatenate((data_incremental_step_scenario_1_y_train, data_incremental_step_scenario_1_y_train_unseen))\n",
    "    data_incremental_step_scenario_1_p_train = np.concatenate((data_incremental_step_scenario_1_p_train, data_incremental_step_scenario_1_p_train_unseen))\n",
    "\n",
    "    data_incremental_step_scenario_1_x_val = np.concatenate((data_incremental_step_scenario_1_x_val, data_incremental_step_scenario_1_x_val_unseen))\n",
    "    data_incremental_step_scenario_1_y_val = np.concatenate((data_incremental_step_scenario_1_y_val, data_incremental_step_scenario_1_y_val_unseen))\n",
    "    data_incremental_step_scenario_1_p_val = np.concatenate((data_incremental_step_scenario_1_p_val, data_incremental_step_scenario_1_p_val_unseen))\n",
    "\n",
    "    print(f\"Total Data: Seen: {len(data_seen_classes_scenario_1_x)}, Unseen: {len(data_unseen_classes_scenario_1_x)}\")\n",
    "    print(\"Inital\")\n",
    "    print(f\"Train: Len: {len(data_initial_step_scenario_1_x_train)} , Unique: {len(np.unique(data_initial_step_scenario_1_y_train))} People: {np.unique(data_initial_step_scenario_1_p_train)}\")\n",
    "    print(f\"Val: Len: {len(data_initial_step_scenario_1_x_val)}, Unique: {len(np.unique(data_initial_step_scenario_1_y_val))} People: {np.unique(data_initial_step_scenario_1_p_val)}\") \n",
    "    print(f\"Test: Len: {len(data_initial_step_x_test)}, Unique: {len(np.unique(data_initial_step_y_test))} People: {np.unique(data_initial_step_p_test)}\")\n",
    "\n",
    "    print(\"Incremantal\")\n",
    "    \n",
    "    print(f\"Train: Len: {len(data_incremental_step_scenario_1_x_train)} , Unique: {len(np.unique(data_incremental_step_scenario_1_y_train))}, People: {np.unique(data_incremental_step_scenario_1_p_train)} \")\n",
    "    print(f\"Val: Len: {len(data_incremental_step_scenario_1_x_val)}, Unique: {len(np.unique(data_incremental_step_scenario_1_y_val))}, People: {np.unique(data_incremental_step_scenario_1_p_val)} \")    \n",
    "    print(f\"Test: Len: {len(data_incremental_step_x_test)} , Unique: {len(np.unique(data_incremental_step_y_test))}, People: {np.unique(data_incremental_step_p_test)}\")\n",
    "    \n",
    "\n",
    "    # Saving the necessary files\n",
    "    windowlen = np.array(data_incremental_step_scenario_1_x_train).shape[1]\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_x_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_y_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_p_train)\n",
    "\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_x_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_y_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_p_val)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_x_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_y_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_p_test)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_x_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_y_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_p_train)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_x_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_y_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_p_val)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_x_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_y_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_p_test)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n",
    "\n",
    "print(le.fit_transform(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"personid\":data_p, \"activityid\":data_y})\n",
    "activities_by_person = df.groupby('personid')['activityid'].nunique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personid</th>\n",
       "      <th>activityid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  personid  activityid\n",
       "0        1          12\n",
       "1       10          12\n",
       "2        2          12\n",
       "3        3          12\n",
       "4        4          12\n",
       "5        5          12\n",
       "6        6          12\n",
       "7        7          12\n",
       "8        8          12\n",
       "9        9          12"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities_by_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27337"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.load(r'C:\\Users\\Dhruv\\Desktop\\CGCD-TTL\\CGCD-main\\src\\HAR_data\\realworld\\data_incremental_step_scenario_1_p_test_windowLen_100_standardized_True_fold0.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
